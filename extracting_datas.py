# -*- coding: utf-8 -*-
"""Extracting_datas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vCCplKOR5RLviuwrv7Sul7mzptjEy6XA
"""

from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex
from llama_parse import LlamaParse
from dotenv import load_dotenv
import nest_asyncio
import json
import os

# Load environment variables and apply nest_asyncio
load_dotenv()
nest_asyncio.apply()

app = FastAPI()

# Initialize OpenAI embeddings globally
embed_model = OpenAIEmbedding()

# Define a list of allowed origins for CORS. Allow all origins
origins = ["*"]

# Add the CORSMiddleware to the FastAPI application
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define a folder to save uploaded files
UPLOAD_FOLDER = './uploaded_files'

def save_file(file: UploadFile):
    """Save the uploaded file locally and return the file path."""
    try:
        # Ensure the directory exists
        if not os.path.exists(UPLOAD_FOLDER):
            os.makedirs(UPLOAD_FOLDER)
        # Construct the full file path
        file_path = os.path.join(UPLOAD_FOLDER, file.filename)
        # Save the file
        with open(file_path, 'wb') as f:
            f.write(file.file.read())  # Write the file content to disk
        return file_path
    except Exception as e:
        raise ValueError(f"Error saving file locally: {e}")

#Parsing the document , extracting the data
def parse_document(file_path: str):
    """Parse the document content using LlamaParse."""
    try:
        llama_doc = LlamaParse(result_type="markdown").load_data(file_path)
        return llama_doc
    except Exception as e:
        raise ValueError(f"Error parsing document: {e}")

#Creating vector index
def create_vector_index(llama_doc):
    """Create a vector index from the parsed document."""
    try:
        vectorindex = VectorStoreIndex.from_documents(llama_doc, embed_model=embed_model)
        return vectorindex
    except Exception as e:
        raise ValueError(f"Error creating vector index: {e}")

#Apply prompt enginner to create a perfect query
def define_extraction_query():
    """Define the question for extracting table-like structures."""
    question = '''
    Extract and format all table-like structures from the provided document, following these guidelines:

    1. Identify all potential table structures, including those without clear headers or borders.
    2. For each identified table-like structure:
       a. Determine the most likely column headers based on context and data patterns.
       b. If column name and headers are missing, automatically generate column names and headers based on context and data patterns.
       c. Extract all data rows, aligning them with the determined headers.
       d. Format as a list of dictionaries, where each dictionary represents a row and keys are the determined column headers.

    Output format:
    - Single line JSON array of objects: [{"table_name": "Table 1", "data": [{"column1": "value1", "column2": "value2", ...}, {...}]}, {"table_name": "Table 2", "data": [...]}]
    - No new lines, no additional text before or after the JSON.

    Critical requirements:
    1. Replace any missing or unclear data with "N/A".
    2. Include all identified fields in every row, using "N/A" for unavailable data.
    3. Maintain consistent column names within each
    '''
    return question

#Function to extract able data step by step
async def extract_table_data(file: UploadFile):
    """Extract table-like structures from the uploaded document."""

    # Save the uploaded file locally
    file_path = save_file(file)

    # Load and parse the document
    llama_doc = parse_document(file_path)

    # Create vector index from the parsed document
    vectorindex = create_vector_index(llama_doc)

    # Define the query for table extraction
    question = define_extraction_query()

    # Execute the query using the vector index
    query_engine = vectorindex.as_query_engine()
    response = query_engine.query(question)

    # Ensure the response is correctly deserialized
    if isinstance(response.response, str):
        try:
            json_response = json.loads(response.response)  # Deserialize JSON string to a Python object
        except json.JSONDecodeError as e:
            raise ValueError(f"Error parsing JSON response: {e}")
    else:
        json_response = response.response  # Already a proper Python object

    return json_response


@app.post("/extract_tables")
async def extract_tables(file: UploadFile = File(...)):
    """API endpoint to extract tables from a document."""
    try:
        table_data = await extract_table_data(file)
        return table_data
        #return JSONResponse(content=table_data, media_type="application/json")
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.get('/')
async def root():
    return {"message": "Welcome to extraction of data!!!"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)